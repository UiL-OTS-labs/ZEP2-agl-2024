
import condition;


enum GrammarType {
    GRAM_L1, // Grammar compatible with L1
    GRAM_L2  // Grammar compatible with L2
};

enum ResponseType {
    FIRST,
    SECOND
};


// Item table record.
record TrainItem
{
    int         id;             // item id
    int         nth_rep;        // the number of repetition.
    CondEntropy entropy;        // entropy of item
    CondLang    language;       // L1 or L2
    string      sndfn;          // stimulus word sound filename
};

void print_train_items(TrainItem[] items, bool header) {
    int i = 0;
    if (header)
        println("id nth_repetition entropy language sound_file");

    while (i < items.size) {
        TrainItem it = items[i];
        println("" + it.id + " "
                   + it.nth_rep + " "
                   + it.entropy + " "
                   + it.language + " "
                   + it.sndfn
               );
        i++;
    }
}


/*
 * The training stimuli are presented in these 3 lists, each
 * list corresponds to a block.
 */
TrainItem[] training_items1 = {};
TrainItem[] training_items2 = {};
TrainItem[] training_items3 = {};


// Training items of language 1 with low entropy
TrainItem[] items_low_l1 =
{
    { 1, 0, EN_LOW, LANG_L1, "rak_bap_toef"},
    { 2, 0, EN_LOW, LANG_L1, "rak_ves_toef"},
    { 3, 0, EN_LOW, LANG_L1, "sot_bap_jik"},
    { 4, 0, EN_LOW, LANG_L1, "sot_ves_jik"},
    { 5, 0, EN_LOW, LANG_L1, "tep_bap_lut"},
    { 6, 0, EN_LOW, LANG_L1, "tep_ves_lut"},
    { 7, 0, EN_LOW, LANG_L1, "rak_huf_toef"},
    { 8, 0, EN_LOW, LANG_L1, "rak_keg_toef"},
    { 9, 0, EN_LOW, LANG_L1, "sot_huf_jik"},
    {10, 0, EN_LOW, LANG_L1, "sot_keg_jik"},
    {11, 0, EN_LOW, LANG_L1, "tep_huf_lut"},
    {12, 0, EN_LOW, LANG_L1, "tep_keg_lut"},
};

// Training items of language 1 with high entropy
TrainItem[] items_high_l1 =
{
    { 1, 0, EN_HIGH, LANG_L1, "rak_bap_toef"},
    { 2, 0, EN_HIGH, LANG_L1, "rak_ves_toef"},
    { 3, 0, EN_HIGH, LANG_L1, "sot_bap_jik"},
    { 4, 0, EN_HIGH, LANG_L1, "sot_ves_jik"},
    { 5, 0, EN_HIGH, LANG_L1, "tep_bap_lut"},
    { 6, 0, EN_HIGH, LANG_L1, "tep_ves_lut"},
    { 7, 0, EN_HIGH, LANG_L1, "rak_huf_toef"},
    { 8, 0, EN_HIGH, LANG_L1, "rak_mip_toef"},
    { 9, 0, EN_HIGH, LANG_L1, "rak_nit_toef"},
    {10, 0, EN_HIGH, LANG_L1, "rak_pem_toef"},
    {11, 0, EN_HIGH, LANG_L1, "rak_wop_toef"},
    {12, 0, EN_HIGH, LANG_L1, "sot_huf_jik"},
    {13, 0, EN_HIGH, LANG_L1, "sot_mip_jik"},
    {14, 0, EN_HIGH, LANG_L1, "sot_nit_jik"},
    {15, 0, EN_HIGH, LANG_L1, "sot_pem_jik"},
    {16, 0, EN_HIGH, LANG_L1, "sot_wop_jik"},
    {17, 0, EN_HIGH, LANG_L1, "tep_huf_lut"},
    {18, 0, EN_HIGH, LANG_L1, "tep_mip_lut"},
    {19, 0, EN_HIGH, LANG_L1, "tep_nit_lut"},
    {20, 0, EN_HIGH, LANG_L1, "tep_pem_lut"},
    {21, 0, EN_HIGH, LANG_L1, "tep_wop_lut"},
    {22, 0, EN_HIGH, LANG_L1, "rak_bif_toef"},
    {23, 0, EN_HIGH, LANG_L1, "rak_dul_toef"},
    {24, 0, EN_HIGH, LANG_L1, "rak_hog_toef"},
    {25, 0, EN_HIGH, LANG_L1, "rak_jal_toef"},
    {26, 0, EN_HIGH, LANG_L1, "rak_keg_toef"},
    {27, 0, EN_HIGH, LANG_L1, "rak_kof_toef"},
    {28, 0, EN_HIGH, LANG_L1, "rak_naf_toef"},
    {29, 0, EN_HIGH, LANG_L1, "rak_nup_toef"},
    {30, 0, EN_HIGH, LANG_L1, "rak_zuk_toef"},
    {31, 0, EN_HIGH, LANG_L1, "sot_bif_jik"},
    {32, 0, EN_HIGH, LANG_L1, "sot_dul_jik"},
    {33, 0, EN_HIGH, LANG_L1, "sot_hog_jik"},
    {34, 0, EN_HIGH, LANG_L1, "sot_jal_jik"},
    {35, 0, EN_HIGH, LANG_L1, "sot_keg_jik"},
    {36, 0, EN_HIGH, LANG_L1, "sot_kof_jik"},
    {37, 0, EN_HIGH, LANG_L1, "sot_naf_jik"},
    {38, 0, EN_HIGH, LANG_L1, "sot_nup_jik"},
    {39, 0, EN_HIGH, LANG_L1, "sot_zuk_jik"},
    {40, 0, EN_HIGH, LANG_L1, "tep_bif_lut"},
    {41, 0, EN_HIGH, LANG_L1, "tep_dul_lut"},
    {42, 0, EN_HIGH, LANG_L1, "tep_hog_lut"},
    {43, 0, EN_HIGH, LANG_L1, "tep_jal_lut"},
    {44, 0, EN_HIGH, LANG_L1, "tep_keg_lut"},
    {45, 0, EN_HIGH, LANG_L1, "tep_kof_lut"},
    {46, 0, EN_HIGH, LANG_L1, "tep_naf_lut"},
    {47, 0, EN_HIGH, LANG_L1, "tep_nup_lut"},
    {48, 0, EN_HIGH, LANG_L1, "tep_zuk_lut"}
}

// Training items of language 2 with low entropy
TrainItem[] items_low_l2 =
{
    { 1, 0, EN_LOW, LANG_L2, "rak_bap_lut"},
    { 3, 0, EN_LOW, LANG_L2, "rak_ves_lut"},
    { 4, 0, EN_LOW, LANG_L2, "sot_bap_toef"},
    { 6, 0, EN_LOW, LANG_L2, "sot_ves_toef"},
    { 7, 0, EN_LOW, LANG_L2, "tep_bap_jik"},
    { 9, 0, EN_LOW, LANG_L2, "tep_ves_jik"},
    {10, 0, EN_LOW, LANG_L2, "rak_huf_lut"},
    {11, 0, EN_LOW, LANG_L2, "rak_keg_lut"},
    {12, 0, EN_LOW, LANG_L2, "sot_huf_toef"},
    {13, 0, EN_LOW, LANG_L2, "sot_keg_toef"},
    {14, 0, EN_LOW, LANG_L2, "tep_huf_jik"},
    {15, 0, EN_LOW, LANG_L2, "tep_keg_jik"},
};


// Training items of language 2 with high entropy
TrainItem[] items_high_l2 =
{
    { 1, 0,  EN_HIGH, LANG_L2, "rak_bap_lut"},
    { 2, 0,  EN_HIGH, LANG_L2, "rak_ves_lut"},
    { 3, 0,  EN_HIGH, LANG_L2, "sot_bap_toef"},
    { 4, 0,  EN_HIGH, LANG_L2, "sot_ves_toef"},
    { 5, 0,  EN_HIGH, LANG_L2, "tep_bap_jik"},
    { 6, 0,  EN_HIGH, LANG_L2, "tep_ves_jik"},
    { 7, 0, EN_HIGH, LANG_L2, "rak_huf_lut"},
    { 8, 0, EN_HIGH, LANG_L2, "rak_mip_lut"},
    { 9, 0, EN_HIGH, LANG_L2, "rak_nit_lut"},
    {10, 0, EN_HIGH, LANG_L2, "rak_pem_lut"},
    {11, 0, EN_HIGH, LANG_L2, "rak_wop_lut"},
    {12, 0, EN_HIGH, LANG_L2, "sot_huf_toef"},
    {13, 0, EN_HIGH, LANG_L2, "sot_mip_toef"},
    {14, 0, EN_HIGH, LANG_L2, "sot_nit_toef"},
    {15, 0, EN_HIGH, LANG_L2, "sot_pem_toef"},
    {16, 0, EN_HIGH, LANG_L2, "sot_wop_toef"},
    {17, 0, EN_HIGH, LANG_L2, "tep_huf_jik"},
    {18, 0, EN_HIGH, LANG_L2, "tep_mip_jik"},
    {19, 0, EN_HIGH, LANG_L2, "tep_nit_jik"},
    {20, 0, EN_HIGH, LANG_L2, "tep_pem_jik"},
    {21, 0, EN_HIGH, LANG_L2, "tep_wop_jik"},
    {22, 0, EN_HIGH, LANG_L2, "rak_bif_lut"},
    {23, 0, EN_HIGH, LANG_L2, "rak_dul_lut"},
    {24, 0, EN_HIGH, LANG_L2, "rak_hog_lut"},
    {25, 0, EN_HIGH, LANG_L2, "rak_jal_lut"},
    {26, 0, EN_HIGH, LANG_L2, "rak_keg_lut"},
    {27, 0, EN_HIGH, LANG_L2, "rak_kof_lut"},
    {28, 0, EN_HIGH, LANG_L2, "rak_naf_lut"},
    {29, 0, EN_HIGH, LANG_L2, "rak_nup_lut"},
    {30, 0, EN_HIGH, LANG_L2, "rak_zuk_lut"},
    {31, 0, EN_HIGH, LANG_L2, "sot_bif_toef"},
    {32, 0, EN_HIGH, LANG_L2, "sot_dul_toef"},
    {33, 0, EN_HIGH, LANG_L2, "sot_hog_toef"},
    {34, 0, EN_HIGH, LANG_L2, "sot_jal_toef"},
    {35, 0, EN_HIGH, LANG_L2, "sot_keg_toef"},
    {36, 0, EN_HIGH, LANG_L2, "sot_kof_toef"},
    {37, 0, EN_HIGH, LANG_L2, "sot_naf_toef"},
    {38, 0, EN_HIGH, LANG_L2, "sot_nup_toef"},
    {39, 0, EN_HIGH, LANG_L2, "sot_zuk_toef"},
    {40, 0, EN_HIGH, LANG_L2, "tep_bif_jik"},
    {41, 0, EN_HIGH, LANG_L2, "tep_dul_jik"},
    {42, 0, EN_HIGH, LANG_L2, "tep_hog_jik"},
    {43, 0, EN_HIGH, LANG_L2, "tep_jal_jik"},
    {44, 0, EN_HIGH, LANG_L2, "tep_keg_jik"},
    {45, 0, EN_HIGH, LANG_L2, "tep_kof_jik"},
    {46, 0, EN_HIGH, LANG_L2, "tep_naf_jik"},
    {47, 0, EN_HIGH, LANG_L2, "tep_nup_jik"},
    {48, 0, EN_HIGH, LANG_L2, "tep_zuk_jik"}
}

void append_train_item(TrainItem[] array, TrainItem item) {
    array.size = array.size + 1;
    array[array.size - 1] = item;
}

void append_test_item(TestItem[] array, TestItem item) {
    array.size = array.size + 1;
    array[array.size - 1] = item;
}

/*
 * Distributes all trainings items over the three separate lists/blocks
 */
void prepare_training_lists(TrainItem[] input) {
    TrainItem[] master_list = {};
    TrainItem[] current_list;
    int nth_rep = 0;
    int i = 0;
    int start;
    int end;
    while (master_list.size < TOT_TRAININGS_ITEMS) {
        i = 0;
        while(i < input.size && i < NUM_TRAININGS_ITEMS) {
            TrainItem item = input[i];
            item.nth_rep = nth_rep;
            append_train_item(master_list, item);
            i++;
        }
        nth_rep++;
    }
    if (master_list.size != TOT_TRAININGS_ITEMS) {
        print_error("The number of trainings items is " + master_list.size +". "); 
        print_error("We expected it to be " + TOT_TRAININGS_ITEMS + "\n");
    }

    // prepare list 1
    current_list = training_items1;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 0;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_train_item(current_list, master_list[i]);
        i++;
    }
    if (current_list.size != NUM_TRAININGS_ITEMS) {
        print_error("block1 The number of trainings items is " + current_list.size +". "); 
        print_error("We expected it to be " + NUM_TRAININGS_ITEMS + "\n");
    }
    
    // prepare list 2
    current_list = training_items2;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 1;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_train_item(current_list, master_list[i]);
        i++;
    }
    if (current_list.size != NUM_TRAININGS_ITEMS) {
        print_error("block2 The number of trainings items is " + current_list.size +". "); 
        print_error("We expected it to be " + NUM_TRAININGS_ITEMS + "\n");
    }
    
    // prepare list 3
    current_list = training_items3;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 2;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_train_item(current_list, master_list[i]);
        i++;
    }
    if (current_list.size != NUM_TRAININGS_ITEMS) {
        print_error("block3 The number of trainings items is " + current_list.size +". "); 
        print_error("We expected it to be " + NUM_TRAININGS_ITEMS + "\n");
    }
}

/*
 * Selects the right source list for the current conditions and
 * prepares the training lists.
 */
void prepare_training_items() {

    TrainItem[] training_items;

    if (chosen_entropy == EN_HIGH) {
        if (chosen_language == LANG_L1)
            training_items = items_high_l1;
        else 
            training_items = items_high_l2;
    }
    else {
        if (chosen_language == LANG_L1)
            training_items = items_low_l1;
        else
            training_items = items_low_l2;
    }

    prepare_training_lists(training_items);
}

/*
 * Test items
 */
record TestItem {
    int         id;
    CondList    list_ab;
    GrammarType g1;
    string      fn1;
    GrammarType g2;
    string      fn2;
};

void print_test_item(TestItem item) {
    println(string(item.id) + " " +
            string(item.list_ab) + " " +
            item.g1  + " " +
            item.fn1 + " " +
            item.g2  + " " +
            item.fn2
            );
}

void print_test_items(TestItem[] items, bool header) {
    int i = 0;
    if (header)
        println("id list_ab grammar1 file1 gramar2 file2");

    while (i < items.size) {
        print_test_item(items[i]);
        i++;
    }
}

/*
 * Test items belonging to list A
 */

TestItem[] test_items_fam_a = {
    { 1, LIST_A, GRAM_L1, "rak_bap_toef", GRAM_L2, "rak_bap_lut"},
    { 2, LIST_A, GRAM_L1, "rak_huf_toef", GRAM_L2, "rak_huf_lut"},
    { 3, LIST_A, GRAM_L1, "rak_ves_toef", GRAM_L2, "rak_ves_lut"},
    { 4, LIST_A, GRAM_L1, "sot_bap_jik",  GRAM_L2, "sot_bap_toef"},
    { 5, LIST_A, GRAM_L1, "sot_huf_jik",  GRAM_L2, "sot_huf_toef"},
    { 6, LIST_A, GRAM_L1, "sot_ves_jik",  GRAM_L2, "sot_ves_toef"},
    { 7, LIST_A, GRAM_L1, "tep_bap_lut",  GRAM_L2, "tep_bap_jik"},
    { 8, LIST_A, GRAM_L1, "tep_huf_lut",  GRAM_L2, "tep_huf_jik"},
    { 9, LIST_A, GRAM_L1, "tep_ves_lut",  GRAM_L2, "tep_ves_jik"},
    {10, LIST_A, GRAM_L1, "rak_keg_toef", GRAM_L2, "rak_keg_lut"},
    {11, LIST_A, GRAM_L1, "sot_keg_jik",  GRAM_L2, "sot_keg_toef"},
    {12, LIST_A, GRAM_L1, "tep_keg_lut",  GRAM_L2, "tep_keg_jik"}
};

TestItem[] test_items_new_a = {
    { 1, LIST_A, GRAM_L1, "rak_bug_toef", GRAM_L2, "rak_bug_lut"},
    { 2, LIST_A, GRAM_L1, "rak_gak_toef", GRAM_L2, "rak_gak_lut"},
    { 3, LIST_A, GRAM_L1, "rak_zim_toef", GRAM_L2, "rak_zim_lut"},
    { 4, LIST_A, GRAM_L1, "sot_bug_jik",  GRAM_L2, "sot_bug_toef"},
    { 5, LIST_A, GRAM_L1, "sot_gak_jik",  GRAM_L2, "sot_gak_toef"},
    { 6, LIST_A, GRAM_L1, "sot_zim_jik",  GRAM_L2, "sot_zim_toef"},
    { 7, LIST_A, GRAM_L1, "tep_bug_lut",  GRAM_L2, "tep_bug_jik"},
    { 8, LIST_A, GRAM_L1, "tep_gak_lut",  GRAM_L2, "tep_gak_jik"},
    { 9, LIST_A, GRAM_L1, "tep_zim_lut",  GRAM_L2, "tep_zim_jik"},
    {10, LIST_A, GRAM_L1, "rak_dos_toef", GRAM_L2, "rak_dos_lut"},
    {11, LIST_A, GRAM_L1, "sot_dos_jik",  GRAM_L2, "sot_dos_toef"},
    {12, LIST_A, GRAM_L1, "tep_dos_lut",  GRAM_L2, "tep_dos_jik"}
};

/*
 * Test items belonging to list B
 */
TestItem[] test_items_fam_b = {
   { 1, LIST_B, GRAM_L1, "rak_bap_toef", GRAM_L2, "sot_bap_toef"},
   { 2, LIST_B, GRAM_L1, "rak_huf_toef", GRAM_L2, "sot_huf_toef"},
   { 3, LIST_B, GRAM_L1, "rak_ves_toef", GRAM_L2, "sot_ves_toef"},
   { 4, LIST_B, GRAM_L1, "sot_bap_jik",  GRAM_L2, "tep_bap_jik"},
   { 5, LIST_B, GRAM_L1, "sof_huf_jik",  GRAM_L2, "tep_huf_jik"},
   { 6, LIST_B, GRAM_L1, "sot_ves_jik",  GRAM_L2, "tep_ves_jik"},
   { 7, LIST_B, GRAM_L1, "tep_bap_lut",  GRAM_L2, "rak_bap_lut"},
   { 8, LIST_B, GRAM_L1, "tep_huf_lut",  GRAM_L2, "rak_huf_lut"},
   { 9, LIST_B, GRAM_L1, "tep_ves_lut",  GRAM_L2, "rak_ves_lut"},
   {10, LIST_B, GRAM_L1, "rak_keg_toef", GRAM_L2, "sot_keg_toef"},
   {11, LIST_B, GRAM_L1, "sot_keg_jik",  GRAM_L2, "tap_keg_jik"},
   {12, LIST_B, GRAM_L1, "tep_keg_lut",  GRAM_L2, "rak_keg_lut"}
};

TestItem[] test_items_new_b = {
   { 1, LIST_B, GRAM_L1, "rak_bug_toef", GRAM_L2, "sot_bug_toef"},
   { 2, LIST_B, GRAM_L1, "rak_gak_toef", GRAM_L2, "sot_gak_toef"},
   { 3, LIST_B, GRAM_L1, "rak_zim_toef", GRAM_L2, "sot_zim_toef"},
   { 4, LIST_B, GRAM_L1, "sot_bug_jik",  GRAM_L2, "tep_bug_jik"},
   { 5, LIST_B, GRAM_L1, "sot_gak_jik",  GRAM_L2, "tep_gak_jik"},
   { 6, LIST_B, GRAM_L1, "sot_zim_jik",  GRAM_L2, "tep_zim_jik"},
   { 7, LIST_B, GRAM_L1, "tep_bug_lut",  GRAM_L2, "rak_bug_lut"},
   { 8, LIST_B, GRAM_L1, "tep_gak_lut",  GRAM_L2, "rak_gak_lut"},
   { 9, LIST_B, GRAM_L1, "tep_zim_lut",  GRAM_L2, "rak_zim_lut"},
   {10, LIST_B, GRAM_L1, "rak_dos_toef", GRAM_L2, "sot_dos_toef"},
   {11, LIST_B, GRAM_L1, "sot_dos_jik",  GRAM_L2, "tep_dos_jik"},
   {12, LIST_B, GRAM_L1, "tep_dos_lut",  GRAM_L2, "rak_dos_lut"}
};

TestItem [] test_items1 = {};
TestItem [] test_items2 = {};
TestItem [] test_items3 = {};
                
TestItem swap_words_in_test_item(TestItem item) {

    string      fntemp = item.fn1;
    GrammarType gtemp  = item.g1;

    item.fn1 = item.fn2;
    item.g1  = item.g2;
    item.fn2 = fntemp;
    item.g2  = gtemp;

    return item;
}

/*
 * prepares the testitems
 */
void prepare_test_stims(TestItem[] test_items) {
    int[] swap = {}; // Nope initializing with {1 ,1, ..., 0}; is a syntax error...
    swap.size = TOT_TEST_ITEMS;

    /*The first and second word are swapped when swap is one*/
    swap[0] = 1;
    swap[1] = 1;
    swap[2] = 1;
    swap[3] = 1;
    swap[4] = 1;
    swap[5] = 1;
    swap[6] = 0;
    swap[7] = 0;
    swap[8] = 0;
    swap[9] = 0;
    swap[10] = 0;
    swap[11] = 0;
    swap.shuffle(0, -1);
    
    int i = 0;

    while (i < TOT_TEST_ITEMS) {
        TestItem item = test_items[i];

        if (swap[i] == 1) {
            item = swap_words_in_test_item(item);
        }

        if (i % 3 == 0)
            append_test_item(test_items1, item);
        else if (i % 3 == 1)
            append_test_item(test_items2, item);
        else
            append_test_item(test_items3, item);

        i++;
    }
}

/*
 * Prepares the list for the three test blocks.
 */
void prepare_test_lists () {

    // shuffle the order of the test items.
    test_items_fam_a.shuffle(0,-1);
    test_items_new_a.shuffle(0,-1);
    test_items_fam_b.shuffle(0,-1);
    test_items_new_b.shuffle(0,-1);

    if (chosen_test_stim == STIM_FAMILIAR) {
        if (chosen_list == LIST_A)
            prepare_test_stims(test_items_fam_a);
        else
            prepare_test_stims(test_items_fam_b);
    }
    else {
        if (chosen_list == LIST_A)
            prepare_test_stims(test_items_new_a);
        else
            prepare_test_stims(test_items_new_b);
    }
}
